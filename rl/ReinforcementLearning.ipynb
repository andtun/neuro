{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с учителем\n",
    "\n",
    "Если у нас есть набор примеров с правильными ответами, то мы используем эту выборку для обучения нашей модели, а после обучения, применяем её к неразмеченным данным. Именно этот подход мы использовали, когда обучали классификатор для MNIST, подавая на вход сети картинки с изображениями рукописных цифр и считая градиент для подстройки весов на основе разницы между известным лэйблом цифры и выходом нейросети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение без учителя\n",
    "\n",
    "В некоторых случаях у нас нет размеченных данных, на которых мы могли бы заранее обучить модель. Но, при решении некоторых задач, можно обойтись без размеченной выборки. Примером такой задачи является задача кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением\n",
    "\n",
    "В некоторых случаях существующие методы обучения без учителя нам не подходят. В то же время у нас нет возможности создать качественную обучающую выборку. При этом мы можем постфактум оценить действия нашей модели и использовать эту оценку подстроить модель так, чтобы она чаще совершала желательные действия и реже - нежелательные. В литературе такую оценку называют вознаграждением (reward), а обучение строится таким образом, чтобы это модель стремилась максимизироавть получаемое вознаграждение.\n",
    "\n",
    "### Терминология: Агент и среда\n",
    "\n",
    "Агент и среда - ключевые понятия в обучении с подкрелением.\n",
    "\n",
    "**Агент** - программа, принимающая решение о дальнейших действиях на основе информации о состоянии среды.\n",
    "\n",
    "**Среда** - это мир, в котором агент должен \"выживать\", т.е. всё, с чем агент может прямо или косвенно взаимодействовать. Среда обладает состоянием (State), агент может влиять на среду, совершая какие-то действия (Actions), переводя среду при этом из одного состояния в другое и получая какое-то вознаграждение. Среда описывается пространством возможных состояний. Конкретное состояние - вектор в этом пространстве.\n",
    "\n",
    "<img src=\"agend_and_environment.gif\">\n",
    "\n",
    "В зависимости от конкретной задачи, агент может наблюдать либо полное состояние среды, либо только некоторую его часть. Во втором случае, агенту может потребоваться какое-то внутреннее представление полного состояния, которое будет обновляться по мере получения новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наиболее яркие примеры использования обучения с подкреплением\n",
    "- В 2013 году DeepMind публикует статью Playing Atari with Deep Reinforcement Learning, где нейросети обучаются игре в старые игры от Atari, используя анализ изображения.\n",
    "- В 2016 году нейросеть AlphaGO Google DeepMind  обыгрывает одного из сильнейших игроков в Go - Ли Седоля. При обучении AlphaGo использовались партии игры живых людей. Чуть позже будет представлена AlphaGO Zero, обучение которой было полностью построено на игре с самой собой. Новая сеть выиграла у старой со счётом 100:0, причём аппаратные ресурсы сократились с 48 TPU до 4 TPU (Tensor Processing Unit Google).\n",
    "- В 2017 году нейросеть OpenAI 5 успешно участвует в соревновании по игре Dota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация алгоритмов обучения с подкреплением\n",
    "\n",
    "### Model-free / Model-based\n",
    "\n",
    "Model-free не строит модель окружения или функции вознаграждения. Model-based алгоритм пытаетс предсказывать, каким будет следующее состояние окружения или вознаграждение.\n",
    "\n",
    "### Value-based / policy-based\n",
    "\n",
    "Policy-based  методы оптимизируют напрямую функцию принятия решения агента. Стратегия (policy) обычно представлена распределением вероятности доступных действий. Value-based метод оптимизирует оценку вознаграждения для всех действий и выбирает выбирает то-действие, по которому прогнозируется большее значение. Методы, основанные на Policy Gradients лучше работают при большой размерности пространства действий, а Value-based методы, такие, как Deep Q-Learning требуют меньшего количества повторений для сходимости при малой размерности.\n",
    "\n",
    "### On-Policy / Off-Policy\n",
    "\n",
    "Off-policy подход позволяет учиться на исторических данных или на записанных заранее действиях человека. On-policy - только на собственных действиях агента.\n",
    "\n",
    "### Deterministic Policy / Stochastic Policy\n",
    "\n",
    "В зависимости от среды, наша стратегия может быть либо детерминированной - выбираем сразу определённое действие с помощью argmax, либо стохастической, когда мы окончательное решение принимается с помощью генератора случайных чисел на основе распределения вероятности, выданного сетью.\n",
    " \n",
    "\n",
    "- Q-learning\n",
    "- Deep Q-learning (DQN)\n",
    "- Double Deep Q-learning (DDQN)\n",
    "- Proximal Policy Optimization\n",
    "- Rainbow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Недостатки RL\n",
    "\n",
    "## Низкая скорость обучения (sample efficiency)\n",
    "\n",
    "Общая проблема всех алгоритмов обучения с подкреплением - низкая скорость обучения. В то время, как человеку может быть достаточно одного повторения, чтобы выучить какое-то действие, агенту RL требуется десятки тысяч повторений даже в простых задачах. В какой-то степени это связано с несовершенством архитектуры, но самый большой вклад даёт тот факт, что человек может использовать накопленный в прошлом опыт из других областей. Игра Montezuma's Revenge - популярный подопытная среда для RL в последнее время. И яркий пример низкой эффективности повторений  у алгоритмов RL по сравнению с человеком. \n",
    "\n",
    "Челокек, как правило, быстро понимает, что нужно избегать черепа и забрать ключ, гравитация направлена вниз, а падение с большой высоты опасно. Алгоритму же приходиться обучаться с полного нуля. Если же подменить элементы интерфейса на неочевидные для человека, то его sample-efficency тоже сильно падает (хотя всё-равно лучше, чем RL).\n",
    "\n",
    "<img src=\"game_prior.gif\" width=\"700\">\n",
    "\n",
    "<img src=\"game_no_prior.gif\" width=\"700\">\n",
    "\n",
    "Так же важным фактором являются редкие награды. Часто в ходе одного эпизода алгоритм делает множество различных действий, а награду полуает только в конце. Соответственно, веса сети можно обновить только в конце эпизода и нельзя поощерить или наказать конкретные действия внутри эпизода. В итоге требуется большое количество повторений для достижения оптимальных весов.\n",
    "\n",
    "Один из способов улучшить эффективность при редких наградах - reward shaping - модификация функции награды так, чтобы явно поощерялись какие-то действия внутри эпизода. Но качественно сконструировать такую функцию тяжело, а ошибки в ней могут приводить к неожиданным эффектам:\n",
    "\n",
    "<img src=\"coastrunner.gif\" width=\"700\">\n",
    "\n",
    "В гонке лодок агент получал вознаграждене не только за победу в гонке, но и за сбор всяких ништяков. В итоге он решил, что гонка не очень-то и нужна, достаточно собирать ништяки.\n",
    "\n",
    "<video controls src=\"upsidedown_half_cheetah.mp4\" width=\"700\"> </video>\n",
    "\n",
    "У данного агента мы наблюдаем поподание в локлаьный минимум. Этот агент получает поощерение за набранную скорость. На начальном этапе во время случайного поиска агент обнаружил, что кувыркнуться вперёд даёт хорошее вознаграждение в начале. Постепенно, после нескольких попыток, переворачивание на спину закрепилось, как успешная стратегия. После закрепления такого поведения агент не смог выйти из этого состояния, т.к. оказалось проще научиться двигаться в таком состоянии, чем научиться переворачиваться обратно на ноги.\n",
    "\n",
    "Похожее поведение можно случано получить, если поощерять агента за то, что его ноги оторваны от земли.\n",
    "    \n",
    "<video controls src=\"failed_reacher.mp4\" width=\"700\"> </video>\n",
    "\n",
    "В данном примере случайная инициализация весов получилась такой, что к вращающейся \"конечности\" в каждой точке прикладывалась большая сила. В результате конечность начала быстро вращаться. Сложность избавления от такого поведения заключается в том, что для того, чтобы отступить от такой стратегии, нужно путём исследования случайных действий предпринять несколько попыток, когда робот не будет вращаться, чтобы такие действия могли закрепиться. Это возможно, но в данном запуске этого не произошло."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотека Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример окружения Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "envName = 'MountainCar-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Episode #0 ended in 201 steps. total_reward=-200.0\n",
      "\n",
      " Episode #1 ended in 201 steps. total_reward=-200.0\n",
      "\n",
      " Episode #2 ended in 201 steps. total_reward=-200.0\n",
      "\n",
      " Episode #3 ended in 201 steps. total_reward=-200.0\n",
      "\n",
      " Episode #4 ended in 201 steps. total_reward=-200.0\n",
      "\n",
      " Episode #5 ended in 201 steps. total_reward=-200.0\n",
      "\n",
      " Episode #6 ended in 201 steps. total_reward=-200.0\n",
      "\n",
      " Episode #7 ended in 201 steps. total_reward=-200.0\n",
      "\n",
      " Episode #8 ended in 201 steps. total_reward=-200.0\n",
      "\n",
      " Episode #9 ended in 201 steps. total_reward=-200.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(envName)\n",
    "MAX_NUM_EPISODES = 10\n",
    "\n",
    "for episode in range(MAX_NUM_EPISODES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0 # To keep track of the total reward obtained in each episode\n",
    "    step = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        \n",
    "        action = env.action_space.sample()  # Sample random action.\n",
    "                                            # This will be replaced\n",
    "                                            # by our agent's action\n",
    "                                            # when we # start\n",
    "                                            # developing the agent algorithms\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = \\\n",
    "        env.step(action)  # Send the action to the\n",
    "                          # environment and receive       \n",
    "                          # the next_state, reward and\n",
    "                          # whether done or not\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        obs = next_state\n",
    "    print(\"\\n Episode #{} ended in {} steps. total_reward={}\".format(episode, step+1,\n",
    "total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-Learning - это метод основанный на представлении функции полезности Q(s, a) в виде таблицы. Соответственно, такой метод применим только для дискретного набора действий и дискретного количества состояний среды, причём желательно, чтобы число возможных действия и число состояний было небольшим. Ограниченно этот метод можно применить к средам с непрерывным состоянием, если его искусственно дискретизировать.\n",
    "\n",
    "\n",
    "\n",
    "Если ввести функцию дисконтированного будущего вознаграждения:\n",
    "\n",
    "\\\\[ R_t = \\sum_{k=0}^{\\infty}{\\gamma^{k} r_{t+k+1}}, \\quad где \\quad \\gamma \\in (0, 1] \\\\],\n",
    "\n",
    "то можно определить функцию \\\\( Q(s, a) \\\\) как математическое ожидание будущего вознаграждения при выполнении действий \\\\( a \\\\) в состоянии \\\\( s \\\\),\n",
    "\n",
    "\\\\[ Q(S, A) = max_{\\pi} \\mathbb{E} [G_t | S_t = s, A_t = a, \\pi] \\\\]\n",
    "\n",
    "SARSA (State, Action, Reward, State, Action) /* on-policy */:\n",
    "\n",
    "\\\\[ Q(S_t, A_t) = (1 - \\alpha) Q(S_t, A_t) + \\alpha [r_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})] \\\\]\n",
    "\n",
    "\\\\[ Q(S_t, A_t) = Q(S_t, A_t) + \\alpha [r_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)] \\\\]\n",
    "\n",
    "\\\\[ TD_{target} = r + \\gamma Q(S_{t+1},A_{t+1})\\\\]\n",
    "\n",
    "\\\\[ TD_{error} = TD_{target} - Q(S_t, A_t) \\\\]\n",
    "\n",
    "Q-обучение /* off-policy */:\n",
    "\n",
    "Уравнение Беллмана:\n",
    "\n",
    "\\\\[ Q(S_t,A_t) = Q(S_t, A_t) + \\alpha [r_{t+1} +  \\gamma \\cdot max_{a}{Q(S_{t+1},a}) - Q(S_t, A_t) ] \\\\]\n",
    "\n",
    "\\\\[ TD_{target} = r + \\gamma max_{a} Q(S_{t+1},a)\\\\]\n",
    "\n",
    "\\\\[ TD_{error} = TD_{target} - Q(S_t, A_t) \\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#:19999 reward:-160.0 best_reward:-117.0 eps:0.032830999948725846"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "EPSILON_MIN = 0.005\n",
    "MAX_NUM_EPISODES = 20000\n",
    "MAX_STEPS_PER_EPISODE = 500\n",
    "max_num_steps = MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE\n",
    "EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\n",
    "ALPHA = 0.05 # learning rate\n",
    "GAMMA = 0.95 # Discount factor\n",
    "NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each\n",
    "                        # observation dim\n",
    "\n",
    "class QAgent():\n",
    "    def __init__(self, env):\n",
    "        self.obs_shape = env.observation_space.shape\n",
    "        self.obs_high = env.observation_space.high\n",
    "        self.obs_low = env.observation_space.low\n",
    "        self.obs_bins = NUM_DISCRETE_BINS\n",
    "        self.bin_width = (self.obs_high - self.obs_low) \\\n",
    "            / self.obs_bins\n",
    "        self.action_shape = env.action_space.n\n",
    "        # Create a table to represent the Q-values\n",
    "        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,\n",
    "                          self.action_shape)) # (51 x 51 x 3)\n",
    "        self.alpha = ALPHA\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = 1.0\n",
    "    \n",
    "    \n",
    "    def discretize(self, obs):\n",
    "        return tuple(((obs - self.obs_low) \\\n",
    "                      / self.bin_width).astype(int))\n",
    "    \n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        discretized_obs  = self.discretize(obs)\n",
    "        # Epsilon-Greedy action selection\n",
    "        if self.epsilon > EPSILON_MIN:\n",
    "            self.epsilon -= EPSILON_DECAY\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return np.argmax(self.Q[discretized_obs])\n",
    "        else:  # choose a random action\n",
    "            return np.random.choice([a for a in range(self.action_shape)])\n",
    "        \n",
    "    \n",
    "    def learn(self, obs, action, reward, next_obs):\n",
    "        discretized_obs = self.discretize(obs)\n",
    "        discretized_next_obs = self.discretize(next_obs)\n",
    "        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])\n",
    "        td_error = td_target - self.Q[discretized_obs][action]\n",
    "        self.Q[discretized_obs][action] +=  self.alpha * td_error\n",
    "\n",
    "\n",
    "def train_Q(agent, env):\n",
    "    best_reward = -float('inf')\n",
    "    for episode in range(MAX_NUM_EPISODES):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        total_reward = 0.0\n",
    "        while not done:\n",
    "            action = agent.get_action(obs)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "        best_reward = max(best_reward, total_reward)\n",
    "        print(\"\\rEpisode#:{} reward:{} best_reward:{} eps:{}\"\n",
    "             .format(episode, total_reward, best_reward, agent.epsilon), end=\"\")\n",
    "        \n",
    "    # Return the trained policy\n",
    "    return np.argmax(agent.Q, axis=2)\n",
    "\n",
    "\n",
    "def test_Q(agent, env, policy):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    while not done:\n",
    "        action = policy[agent.discretize(obs)]\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(envName)\n",
    "    agent = QAgent(env)\n",
    "    learned_policy = train_Q(agent, env)\n",
    "    # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "    gym_monitor_path = \"./gym_monitor_output\"\n",
    "    env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n",
    "    for _ in range(1000):\n",
    "        test_Q(agent, env, learned_policy)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks\n",
    "\n",
    "Таблицу, аппроксимирующую нашу функцию качества можно заменить нейросетью, которая будет предсказывать значению Q сразу для всех возможных действий. В этом случае нам нужно определить функцию потерь, от которой мы будем считать градиент.\n",
    "\n",
    "\\\\[ L = {TD_{error}}^2 \\\\]\n",
    "\\\\[ L = (r + \\gamma max_a Q(S_t, a) - Q(S_t, A_t))^2 \\\\]\n",
    "\n",
    "# Улучшения DQN\n",
    "\n",
    "## Experience replay\n",
    "\n",
    "В большинстве окружений информация, получаемая агентом распределена не независимо. Т.е. последовательные наблюдения агента сильно коррелированы между собой (что понятно из интуитивных соображений, т.к. большинство окружений, в которых применяется RL, предполагают, что все изменения в них последовательны). Корреляция примеров ухудшает сходимость стохастического градиентного спуска. Таким образом нам нужен способ, который позволяет улучшить распределение примеров для обучения (устранить или снизить корреляцию между ними). Обычно используется метод **проигрывания опыта (experience replay)**. Суть этого метода в том, что мы сохраняем некоторое количество примеров (состояние, действия, вознаграждение) в специальном буфере и для обучения выбираем случайные мини-батчи из этого буфера.\n",
    "\n",
    "Так же **experience replay** позволяет агенту эффективнее использовать свой прошлый опыт.\n",
    "\n",
    "## Double DQN\n",
    "\n",
    "Одной из проблем Q-Networks является неустойчивость. Часто разность ожидаемых вознаграждений для различных действий близка и поскольку выбор действия производится с помощью argmax, то выброс в данных может привести к тому, что выбираемое действие изменится. Для того, чтобы повысить стабильность используется техника **Target Q-Network**. Суть в том, что мы замораживаем веса нашей сети на фиксированное число шагов и затем используем её для вычисления функции ошибки и обучения второй сети. Периодически копируем из веса рабочей сети в Target Q-Network.\n",
    "В следующем примере мы будем делать это каждый эпизод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "envName = 'MountainCar-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "EPISODES = 0\n",
    "\n",
    "\n",
    "# Double DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = True\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        # с параметрами стоит экспериментировать - тут неоптимальные значения,\n",
    "        # а оптимальные я не помню :(\n",
    "        \n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 0.01\n",
    "        self.epsilon_decay = 0.9991\n",
    "        self.epsilon_min = 0.0001\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 500\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_ddqn3.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_next = self.model.predict(update_target)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_next[i])\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    target_val[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, you can play until 500 time step\n",
    "    env = gym.make(envName)\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DoubleDQNAgent(state_size, action_size)\n",
    "    agent.load_model = 1\n",
    "    \n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        #best_height = -1\n",
    "        #best_velocity = -1\n",
    "        num_actions = 0\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            best_height = next_state[0][0]+0.5\n",
    "            best_velocity = next_state[0][1]\n",
    "            # if an action make the episode end, then gives penalty of -\n",
    "            # последний коэф-т, вероятно, стоит сделать чуть больше\n",
    "            reward = reward + 20*abs(best_velocity) + abs(best_height) + 0.0001*best_height\n",
    "            num_actions += 1\n",
    "            if done: reward += 200 - num_actions\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            agent.train_model()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                #score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_ddqn.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > -90:\n",
    "                    agent.model.save_weights(\"./save_model/cartpole_ddqn3.h5\")\n",
    "                    break\n",
    "\n",
    "        # save the model\n",
    "        if e % 25 == 0:\n",
    "            agent.model.save_weights(\"./save_model/cartpole_ddqn3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        env = gym.make(envName)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = DoubleDQNAgent(state_size, action_size)\n",
    "        agent.load_model = 1\n",
    "        agent.epsilon = 0\n",
    "\n",
    "        # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "        gym_monitor_path = \"./gym_monitor_output\"\n",
    "        env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n",
    "        for _ in range(10):\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                obs = np.reshape(obs, [1, state_size])\n",
    "                total_reward = 0.0\n",
    "                while not done:\n",
    "                    action = agent.get_action(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    next_obs = np.reshape(next_obs, [1, state_size])\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Другие улучшения DQN\n",
    "\n",
    "## Prioritized Experience Replay\n",
    "\n",
    "Минибатчи из памяти выбираются не с равномерным распределением, а добавляем туда больше примеров, в которых предсказанные значения Q сильнее всего отличаются от корректных. Т.е. примеры с максимальным **TD error** получают максимальный приоритет.\n",
    "\n",
    "## Dueling networks\n",
    "\n",
    "Основная идея в том, что мы разделяем нашу сеть на две головы, одна из которых предсказывает абсолютное значение состояния \\\\( V(S) \\\\), а вторая - относительное преимущество одний действий над другими \\\\( A(s, a) = Q(s, a) - V(s) \\\\). Это преимущество называется advantage. Далее из этих двух значений мы собираем нашу Q-функцию, как \\\\( Q(s,a) = V(s) + A(a) \\\\)\n",
    "\n",
    "## Noisy nets\n",
    "\n",
    "Т.к. по мере обучения агент будет стремиться выбирать состояния с максимальным Q, среду уже исследованных, это может помешать ему найти более эффективные состояния, в которых он ещё не было. Одним из решений этой проблемы является использование детерминированной и случайной нейросети, распределение параметров которой так же обучается с помощью градиентного спуска.\n",
    "\n",
    "## Multi-step learning/n-step learning\n",
    "\n",
    "Основная идея в том, чтобы считать функцию ошибки не по двум соседним примерам, а сразу по n. Это позволяет сети лучше запоминать длинные последовательности действий.\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "Детерминированное значение Q заменяется случайным распределением Z с некоторыми параметрами, которые определяются в ходе обучения.\n",
    "\n",
    "# Rainbow\n",
    "\n",
    "State of the art в развитии Q-обучения - набор перечисленных выше твиков. На графике ниже сравнение различных алгоритмов по количеству очков, усреднённое по играм Atari в сравнении со средними результатами человека.\n",
    "\n",
    "<img src=\"rainbow_dqn.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "Мы используем нейросеть, которая получает на вход вектор состояния среды, а на выход сразу выдаёт вектор необходимых действий. Такой подход называется DPG (Deep Policy Gradients) или, в случае детерминированной стратегии, DDPG (Deep Deterministic Policy Gradients). Для подстройки весов в этом случае мы будем использовать градиент:\n",
    "\\\\[ - \\nabla log P(s, a, \\theta) R \\\\]\n",
    "где P - предсказание вероятности действий нейросетью, а R - полученное вознаграждение.\n",
    "\n",
    "Основной минус этого метода - необходимо дожидаться конца эпизода для получения куммулятивного вознаграждения. В отличии от DQN возможна работа с непрерывными действиями, тогда, как DQN может работать только с дискретным набором.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_317 (Dense)            (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dense_318 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 17,283\n",
      "Trainable params: 17,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 44.8492230600759\n",
      "episode: 1   score: 25.056597110564446\n",
      "episode: 2   score: 29.40143011109801\n",
      "episode: 3   score: 34.46112059119001\n",
      "episode: 4   score: 36.13657118208267\n",
      "episode: 5   score: 15.40633528332794\n",
      "episode: 6   score: 20.03913881907044\n",
      "episode: 7   score: 15.400292570835559\n",
      "episode: 8   score: 54.901170949631904\n",
      "episode: 9   score: 17.696015381477316\n",
      "episode: 10   score: 23.052783783276315\n",
      "episode: 11   score: 27.707500639227472\n",
      "episode: 12   score: 46.24077913347028\n",
      "episode: 13   score: 33.859471311843464\n",
      "episode: 14   score: 56.761413466611806\n",
      "episode: 15   score: 19.244676818782633\n",
      "episode: 16   score: 22.981180905467596\n",
      "episode: 17   score: 35.00882571068338\n",
      "episode: 18   score: 60.63475063516722\n",
      "episode: 19   score: 32.1641239920397\n",
      "episode: 20   score: 34.546558626578296\n",
      "episode: 21   score: 23.889248692743376\n",
      "episode: 22   score: 14.062815596415113\n",
      "episode: 23   score: 34.92835822391221\n",
      "episode: 24   score: 19.77303138627912\n",
      "episode: 25   score: 62.601746465150384\n",
      "episode: 26   score: 34.97537248362415\n",
      "episode: 27   score: 26.566615971221623\n",
      "episode: 28   score: 34.925502509111276\n",
      "episode: 29   score: 68.16773108057936\n",
      "episode: 30   score: 40.15058224180818\n",
      "episode: 31   score: 23.589502816185167\n",
      "episode: 32   score: 17.098858955975103\n",
      "episode: 33   score: 60.94064372087453\n",
      "episode: 34   score: 29.549092504986604\n",
      "episode: 35   score: 63.769679728518504\n",
      "episode: 36   score: 19.0607560188986\n",
      "episode: 37   score: 21.5048849474858\n",
      "episode: 38   score: 31.139337482669475\n",
      "episode: 39   score: 36.85621678746318\n",
      "episode: 40   score: 50.701181244230035\n",
      "episode: 41   score: 62.1280077482406\n",
      "episode: 42   score: 25.546120015718703\n",
      "episode: 43   score: 43.14944242720932\n",
      "episode: 44   score: 40.39241774937933\n",
      "episode: 45   score: 35.60594433341842\n",
      "episode: 46   score: 28.973729325132307\n",
      "episode: 47   score: 26.89366790730261\n",
      "episode: 48   score: 26.281573228972775\n",
      "episode: 49   score: 38.290133138039145\n",
      "episode: 50   score: 20.377163953545157\n",
      "episode: 51   score: 34.12035798560283\n",
      "episode: 52   score: 23.2976693924784\n",
      "episode: 53   score: 18.406614167095825\n",
      "episode: 54   score: 52.23693490757715\n",
      "episode: 55   score: 40.49416526112112\n",
      "episode: 56   score: 33.846884306486444\n",
      "episode: 57   score: 22.71316090543492\n",
      "episode: 58   score: 32.43065125046194\n",
      "episode: 59   score: 40.466140808863635\n",
      "episode: 60   score: 49.07919086481011\n",
      "episode: 61   score: 22.896173071907352\n",
      "episode: 62   score: 26.753060156805347\n",
      "episode: 63   score: 63.761739921858144\n",
      "episode: 64   score: 25.860701874717613\n",
      "episode: 65   score: 67.30063314059517\n",
      "episode: 66   score: 35.55647430109911\n",
      "episode: 67   score: 41.076436333028745\n",
      "episode: 68   score: 28.333786238260277\n",
      "episode: 69   score: 19.898047536961137\n",
      "episode: 70   score: 16.019725742195277\n",
      "episode: 71   score: 57.2261534040865\n",
      "episode: 72   score: 86.22715014247342\n",
      "episode: 73   score: 26.568065945340003\n",
      "episode: 74   score: 55.95959622239184\n",
      "episode: 75   score: 15.927289739035306\n",
      "episode: 76   score: 22.95136639291733\n",
      "episode: 77   score: 48.24521842498258\n",
      "episode: 78   score: 24.724150949416384\n",
      "episode: 79   score: 30.003669358017653\n",
      "episode: 80   score: 29.596558712919734\n",
      "episode: 81   score: 39.76987856216901\n",
      "episode: 82   score: 67.64285557872896\n",
      "episode: 83   score: 19.574989160099825\n",
      "episode: 84   score: 30.240986818525\n",
      "episode: 85   score: 29.64127336865452\n",
      "episode: 86   score: 14.373140105621578\n",
      "episode: 87   score: 19.233662184522718\n",
      "episode: 88   score: 52.241672123997986\n",
      "episode: 89   score: 30.649345722854587\n",
      "episode: 90   score: 44.05022112867045\n",
      "episode: 91   score: 19.352126074761415\n",
      "episode: 92   score: 24.17971385173516\n",
      "episode: 93   score: 42.908720669525486\n",
      "episode: 94   score: 28.313813110365\n",
      "episode: 95   score: 66.99992373397731\n",
      "episode: 96   score: 50.453278338721745\n",
      "episode: 97   score: 56.031341733936635\n",
      "episode: 98   score: 32.82037077265651\n",
      "episode: 99   score: 35.92079608625695\n",
      "episode: 100   score: 42.48959719412882\n",
      "episode: 101   score: 33.76841183789017\n",
      "episode: 102   score: 38.56131057175102\n",
      "episode: 103   score: 19.108598636694538\n",
      "episode: 104   score: 13.878465181993278\n",
      "episode: 105   score: 72.06252135321684\n",
      "episode: 106   score: 29.46747885823457\n",
      "episode: 107   score: 30.23193749761732\n",
      "episode: 108   score: 54.30834336317349\n",
      "episode: 109   score: 30.270095125550153\n",
      "episode: 110   score: 18.351574472951892\n",
      "episode: 111   score: 50.670628082198604\n",
      "episode: 112   score: 28.925374900835656\n",
      "episode: 113   score: 41.95680465377979\n",
      "episode: 114   score: 24.334692815097544\n",
      "episode: 115   score: 32.34007725226878\n",
      "episode: 116   score: 47.04393152947737\n",
      "episode: 117   score: 73.82256612397866\n",
      "episode: 118   score: 39.12320126514419\n",
      "episode: 119   score: 54.48514542289262\n",
      "episode: 120   score: 12.714637407231228\n",
      "episode: 121   score: 29.40347278718117\n",
      "episode: 122   score: 27.811125249922302\n",
      "episode: 123   score: 22.606799448817974\n",
      "episode: 124   score: 13.962717404476964\n",
      "episode: 125   score: 33.42996549917373\n",
      "episode: 126   score: 67.37794084267331\n",
      "episode: 127   score: 37.876172528888276\n",
      "episode: 128   score: 48.56929687116982\n",
      "episode: 129   score: 61.292296153834734\n",
      "episode: 130   score: 32.47842248729751\n",
      "episode: 131   score: 40.41366954785235\n",
      "episode: 132   score: 53.868468916037706\n",
      "episode: 133   score: 37.240363924522285\n",
      "episode: 134   score: 30.33886798131123\n",
      "episode: 135   score: 42.55858262604371\n",
      "episode: 136   score: 40.749584592952885\n",
      "episode: 137   score: 27.806121840679168\n",
      "episode: 138   score: 30.453624712081524\n",
      "episode: 139   score: 28.12531011435971\n",
      "episode: 140   score: 22.578973738345\n",
      "episode: 141   score: 44.04610915220196\n",
      "episode: 142   score: 27.94588343087257\n",
      "episode: 143   score: 32.64595584584577\n",
      "episode: 144   score: 14.790102456454768\n",
      "episode: 145   score: 26.644266105257586\n",
      "episode: 146   score: 52.082903476839704\n",
      "episode: 147   score: 39.62977681447358\n",
      "episode: 148   score: 36.117310825801354\n",
      "episode: 149   score: 32.420959951061505\n",
      "episode: 150   score: 20.574482255167563\n",
      "episode: 151   score: 22.562688158317407\n",
      "episode: 152   score: 44.145678879951944\n",
      "episode: 153   score: 47.57816191999746\n",
      "episode: 154   score: 33.33067498784214\n",
      "episode: 155   score: 47.55682178683602\n",
      "episode: 156   score: 32.16305746382611\n",
      "episode: 157   score: 43.123075769027146\n",
      "episode: 158   score: 16.68724645282581\n",
      "episode: 159   score: 69.17410358372324\n",
      "episode: 160   score: 20.41553576136176\n",
      "episode: 161   score: 31.127315237031283\n",
      "episode: 162   score: 38.41745794019932\n",
      "episode: 163   score: 23.898163650972776\n",
      "episode: 164   score: 43.14389283453428\n",
      "episode: 165   score: 52.690352000259836\n",
      "episode: 166   score: 47.614388467084304\n",
      "episode: 167   score: 57.34737841101269\n",
      "episode: 168   score: 31.690965939897012\n",
      "episode: 169   score: 21.196443975311443\n",
      "episode: 170   score: 50.77937658439479\n",
      "episode: 171   score: 32.57393850643042\n",
      "episode: 172   score: 44.818209907996845\n",
      "episode: 173   score: 61.62884137200942\n",
      "episode: 174   score: 36.88551564849321\n",
      "episode: 175   score: 34.003331818345664\n",
      "episode: 176   score: 50.48249319749378\n",
      "episode: 177   score: 23.208932368342523\n",
      "episode: 178   score: 19.029131918746465\n",
      "episode: 179   score: 33.33065734106435\n",
      "episode: 180   score: 44.204040088806494\n",
      "episode: 181   score: 62.44728705154695\n",
      "episode: 182   score: 44.43009128802719\n",
      "episode: 183   score: 40.108167365941604\n",
      "episode: 184   score: 19.115511921332004\n",
      "episode: 185   score: 13.741813193835082\n",
      "episode: 186   score: 44.21795356394087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 187   score: 66.84843415290415\n",
      "episode: 188   score: 50.99842008036938\n",
      "episode: 189   score: 60.21874361504972\n",
      "episode: 190   score: 26.833029615520076\n",
      "episode: 191   score: 26.37228835779946\n",
      "episode: 192   score: 22.871020388653506\n",
      "episode: 193   score: 39.845176036308196\n",
      "episode: 194   score: 29.673551808129\n",
      "episode: 195   score: 60.256590537881564\n",
      "episode: 196   score: 43.40916406779462\n",
      "episode: 197   score: 13.137681664992783\n",
      "episode: 198   score: 21.211943363378705\n",
      "episode: 199   score: 54.51530629856049\n",
      "episode: 200   score: 19.66362288624949\n",
      "episode: 201   score: 26.674727394028384\n",
      "episode: 202   score: 47.89669483711242\n",
      "episode: 203   score: 29.822332529087205\n",
      "episode: 204   score: 28.33584528165718\n",
      "episode: 205   score: 55.92825877351824\n",
      "episode: 206   score: 66.43322165650324\n",
      "episode: 207   score: 48.5215229643786\n",
      "episode: 208   score: 64.02612727752826\n",
      "episode: 209   score: 50.079596555496536\n",
      "episode: 210   score: 32.98004272369793\n",
      "episode: 211   score: 35.612392014040076\n",
      "episode: 212   score: 55.27022548892038\n",
      "episode: 213   score: 26.78015220939126\n",
      "episode: 214   score: 46.26412815123503\n",
      "episode: 215   score: 21.474536502410785\n",
      "episode: 216   score: 22.332541609813436\n",
      "episode: 217   score: 19.955748778092268\n",
      "episode: 218   score: 54.96918380913488\n",
      "episode: 219   score: 13.62395389033129\n",
      "episode: 220   score: 27.836594654228016\n",
      "episode: 221   score: 30.98577936446144\n",
      "episode: 222   score: 30.357627532702786\n",
      "episode: 223   score: 30.70595787299091\n",
      "episode: 224   score: 45.70018668866603\n",
      "episode: 225   score: 46.45690330254608\n",
      "episode: 226   score: 42.101168832201424\n",
      "episode: 227   score: 12.878145906537283\n",
      "episode: 228   score: 31.241744244821042\n",
      "episode: 229   score: 32.52516543892854\n",
      "episode: 230   score: 25.58637233703831\n",
      "episode: 231   score: 16.47049171270046\n",
      "episode: 232   score: 31.350187911364607\n",
      "episode: 233   score: 21.570480878844485\n",
      "episode: 234   score: 15.625876439244848\n",
      "episode: 235   score: 19.9179891974954\n",
      "episode: 236   score: 46.661524681793544\n",
      "episode: 237   score: 77.46414491358271\n",
      "episode: 238   score: 79.75439677207683\n",
      "episode: 239   score: 46.950143326442074\n",
      "episode: 240   score: 40.3074960018277\n",
      "episode: 241   score: 42.065716800020446\n",
      "episode: 242   score: 32.34992664131254\n",
      "episode: 243   score: 56.831484402330766\n",
      "episode: 244   score: 38.58323762019101\n",
      "episode: 245   score: 62.29789916228139\n",
      "episode: 246   score: 61.76490694165312\n",
      "episode: 247   score: 28.83197894945501\n",
      "episode: 248   score: 34.13339414832466\n",
      "episode: 249   score: 24.958903896808888\n",
      "episode: 250   score: 39.95521909853595\n",
      "episode: 251   score: 77.60930898373661\n",
      "episode: 252   score: 38.66229327223832\n",
      "episode: 253   score: 16.93203332047781\n",
      "episode: 254   score: 68.54866289941228\n",
      "episode: 255   score: 62.20259848458741\n",
      "episode: 256   score: 49.666328650737846\n",
      "episode: 257   score: 68.92019843958047\n",
      "episode: 258   score: 16.822015727627292\n",
      "episode: 259   score: 32.17631320639781\n",
      "episode: 260   score: 22.66810093270601\n",
      "episode: 261   score: 18.424908597187223\n",
      "episode: 262   score: 36.00196237941793\n",
      "episode: 263   score: 19.56245963506983\n",
      "episode: 264   score: 27.608615898984116\n",
      "episode: 265   score: 46.61972110551804\n",
      "episode: 266   score: 27.519723833356363\n",
      "episode: 267   score: 47.35743912209843\n",
      "episode: 268   score: 20.75534208346111\n",
      "episode: 269   score: 80.04230250410268\n",
      "episode: 270   score: 39.911559123787804\n",
      "episode: 271   score: 53.69023172495532\n",
      "episode: 272   score: 34.107729615844605\n",
      "episode: 273   score: 34.754272196355586\n",
      "episode: 274   score: 11.427702151093527\n",
      "episode: 275   score: 26.40306217864781\n",
      "episode: 276   score: 41.02628177974451\n",
      "episode: 277   score: 58.97293279915899\n",
      "episode: 278   score: 52.884096603111146\n",
      "episode: 279   score: 28.852878689409888\n",
      "episode: 280   score: 29.30171868028348\n",
      "episode: 281   score: 50.58903243814437\n",
      "episode: 282   score: 29.3286994632909\n",
      "episode: 283   score: 66.47054842732422\n",
      "episode: 284   score: 31.209092470533477\n",
      "episode: 285   score: 30.10984559131641\n",
      "episode: 286   score: 24.4675231128375\n",
      "episode: 287   score: 51.219649007579996\n",
      "episode: 288   score: 11.882317531113745\n",
      "episode: 289   score: 17.249933780667643\n",
      "episode: 290   score: 29.32999394356221\n",
      "episode: 291   score: 45.7623338298743\n",
      "episode: 292   score: 19.02523972959921\n",
      "episode: 293   score: 21.69646798305226\n",
      "episode: 294   score: 32.28192748586758\n",
      "episode: 295   score: 24.415929264466556\n",
      "episode: 296   score: 35.91345542642404\n",
      "episode: 297   score: 41.60437989130963\n",
      "episode: 298   score: 44.38801467490838\n",
      "episode: 299   score: 48.455598149980084\n",
      "episode: 300   score: 55.60070708742102\n",
      "episode: 301   score: 30.126354205223574\n",
      "episode: 302   score: 36.67126869240836\n",
      "episode: 303   score: 29.129820849805867\n",
      "episode: 304   score: 53.37589999521469\n",
      "episode: 305   score: 37.498455935009325\n",
      "episode: 306   score: 27.634512748271796\n",
      "episode: 307   score: 40.97025188813254\n",
      "episode: 308   score: 18.731980190531868\n",
      "episode: 309   score: 10.632797327385655\n",
      "episode: 310   score: 18.58904474031846\n",
      "episode: 311   score: 49.847927758208016\n",
      "episode: 312   score: 23.43485966548626\n",
      "episode: 313   score: 50.65164872261789\n",
      "episode: 314   score: 82.26026522363178\n",
      "episode: 315   score: 39.89082193655619\n",
      "episode: 316   score: 28.63508963558886\n",
      "episode: 317   score: 54.22967010087477\n",
      "episode: 318   score: 97.62948680359709\n",
      "episode: 319   score: 15.417185248973965\n",
      "episode: 320   score: 52.35809328671614\n",
      "episode: 321   score: 36.89081298800832\n",
      "episode: 322   score: 38.165807393746256\n",
      "episode: 323   score: 40.890227094065125\n",
      "episode: 324   score: 38.671878252608266\n",
      "episode: 325   score: 45.379608292658475\n",
      "episode: 326   score: 17.39424577330262\n",
      "episode: 327   score: 28.684702578599065\n",
      "episode: 328   score: 51.84256974094457\n",
      "episode: 329   score: 29.031380953070016\n",
      "episode: 330   score: 25.900117847077592\n",
      "episode: 331   score: 50.86122829340712\n",
      "episode: 332   score: 36.147221178969616\n",
      "episode: 333   score: 18.43734511528652\n",
      "episode: 334   score: 38.11239665986534\n",
      "episode: 335   score: 32.29227840369215\n",
      "episode: 336   score: 28.19752062786379\n",
      "episode: 337   score: 49.558555001138735\n",
      "episode: 338   score: 52.617710424887875\n",
      "episode: 339   score: 35.940015732125815\n",
      "episode: 340   score: 56.79956824411704\n",
      "episode: 341   score: 40.35382630418473\n",
      "episode: 342   score: 64.24243427615734\n",
      "episode: 343   score: 32.45206453851634\n",
      "episode: 344   score: 17.872504063510448\n",
      "episode: 345   score: 23.085346194007016\n",
      "episode: 346   score: 26.644376120922043\n",
      "episode: 347   score: 42.562106312611526\n",
      "episode: 348   score: 12.953311165779292\n",
      "episode: 349   score: 44.781338854147364\n",
      "episode: 350   score: 23.385333996900606\n",
      "episode: 351   score: 37.79999603261454\n",
      "episode: 352   score: 30.971739559689265\n",
      "episode: 353   score: 28.676164664759693\n",
      "episode: 354   score: 26.0537459414556\n",
      "episode: 355   score: 54.16891319641564\n",
      "episode: 356   score: 24.078133112803346\n",
      "episode: 357   score: 49.80174203007028\n",
      "episode: 358   score: 47.29720967593234\n",
      "episode: 359   score: 21.485351135847083\n",
      "episode: 360   score: 30.40539796409646\n",
      "episode: 361   score: 23.846363827289053\n",
      "episode: 362   score: 33.9960651740866\n",
      "episode: 363   score: 11.879962158693239\n",
      "episode: 364   score: 54.200265754438206\n",
      "episode: 365   score: 22.173034195897138\n",
      "episode: 366   score: 50.16001929377762\n",
      "episode: 367   score: 26.081283173970984\n",
      "episode: 368   score: 9.1338392109616\n",
      "episode: 369   score: 10.790913359236267\n",
      "episode: 370   score: 13.704998859267207\n",
      "episode: 371   score: 43.937998297539885\n",
      "episode: 372   score: 17.428077692835608\n",
      "episode: 373   score: 33.565574779416515\n",
      "episode: 374   score: 29.662524143636357\n",
      "episode: 375   score: 30.64259174191739\n",
      "episode: 376   score: 34.10731206961671\n",
      "episode: 377   score: 38.07008897787048\n",
      "episode: 378   score: 28.406540551377777\n",
      "episode: 379   score: 34.50187893197186\n",
      "episode: 380   score: 59.703913251657426\n",
      "episode: 381   score: 22.79681788403647\n",
      "episode: 382   score: 23.024561849738927\n",
      "episode: 383   score: 57.32446374903228\n",
      "episode: 384   score: 34.69029199920761\n",
      "episode: 385   score: 58.188421071481756\n",
      "episode: 386   score: 33.05381972014349\n",
      "episode: 387   score: 39.22823556056072\n",
      "episode: 388   score: 26.513865467974753\n",
      "episode: 389   score: 34.04867330755496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 390   score: 47.86987020633265\n",
      "episode: 391   score: 41.76200520861439\n",
      "episode: 392   score: 80.1206309560824\n",
      "episode: 393   score: 24.59404244362605\n",
      "episode: 394   score: 28.894037449572323\n",
      "episode: 395   score: 65.21939174886671\n",
      "episode: 396   score: 43.729687161596374\n",
      "episode: 397   score: 17.647562968917434\n",
      "episode: 398   score: 21.314690995760287\n",
      "episode: 399   score: 75.0682756823706\n",
      "episode: 400   score: 26.574323073405612\n",
      "episode: 401   score: 54.57566343906918\n",
      "episode: 402   score: 41.05044628566822\n",
      "episode: 403   score: 31.250216023981377\n",
      "episode: 404   score: 30.21857396150943\n",
      "episode: 405   score: 28.694803316582334\n",
      "episode: 406   score: 38.92473014158252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-4873d925081c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;31m# get action for the current state and go one step in environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-4873d925081c>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# using the output of policy network, pick action stochastically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m           callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3275\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3276\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[1;32m-> 3277\u001b[1;33m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[0;32m   3278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.close()\n",
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 501\n",
    "\n",
    "\n",
    "# This is Policy Gradient agent for the Cartpole\n",
    "# In this example, we use DPG algorithm which uses monte-carlo update rule\n",
    "class DPGAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.0005\n",
    "        self.hidden1, self.hidden2 = 128, 128\n",
    "\n",
    "        # create model for policy network\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        # lists for the states, actions and rewards\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_dpg.h5\")\n",
    "\n",
    "    # approximate policy using Neural Network\n",
    "    # state is input and probability of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden1, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "        model.add(Dense(self.hidden2, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='softmax', kernel_initializer='glorot_uniform'))\n",
    "        model.summary()\n",
    "        # Using categorical crossentropy as a loss is a trick to easily\n",
    "        # implement the policy gradient. Categorical cross entropy is defined\n",
    "        # H(p, q) = sum(p_i * log(q_i)). For the action taken, a, you set \n",
    "        # p_a = advantage. q_a is the output of the policy network, which is\n",
    "        # the probability of taking the action a, i.e. policy(s, a). \n",
    "        # All other p_i are zero, thus we have H(p, q) = A * log(policy(s, a))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.model.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # In Policy Gradient, Q function is not available.\n",
    "    # Instead agent uses sample returns for evaluating policy\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save <s, a ,r> of each step\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self):\n",
    "        episode_length = len(self.states)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards(self.rewards)\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        if np.std(discounted_rewards):\n",
    "            discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        update_inputs = np.zeros((episode_length, self.state_size))\n",
    "        advantages = np.zeros((episode_length, self.action_size))\n",
    "\n",
    "        for i in range(episode_length):\n",
    "            update_inputs[i] = self.states[i]\n",
    "            advantages[i][self.actions[i]] = discounted_rewards[i]\n",
    "\n",
    "        self.model.fit(update_inputs, advantages, epochs=1, verbose=0)\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, you can play until 500 time step\n",
    "    env = gym.make(envName)\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # make DPG agent\n",
    "    agent = DPGAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        num_actions = 0\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            best_height = next_state[0][0]+0.5\n",
    "            best_velocity = next_state[0][1]\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = 20*abs(best_velocity) + abs(best_height) + 0.0001*best_height\n",
    "            num_actions += 1\n",
    "            if done: reward += 200 - num_actions\n",
    "            # save the sample <s, a, r> to the memory\n",
    "            agent.append_sample(state, action, reward)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode, agent learns from sample returns\n",
    "                agent.train_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                #score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_dpg.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                \"\"\"if np.mean(scores[-min(10, len(scores)):]) > -10:\n",
    "                    agent.model.save_weights(\"./save_model/cartpole_dpg.h5\")\n",
    "                    break\"\"\"\n",
    "\n",
    "        # save the model\n",
    "        if e % 25 == 0:\n",
    "            agent.model.save_weights(\"./save_model/cartpole_dpg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_243 (Dense)            (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_245 (Dense)            (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        env = gym.make(envName)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = DPGAgent(state_size, action_size)\n",
    "        agent.load_model = 1\n",
    "        agent.epsilon = 0\n",
    "\n",
    "        # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "        gym_monitor_path = \"./gym_monitor_output\"\n",
    "        env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n",
    "        for _ in range(10):\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                obs = np.reshape(obs, [1, state_size])\n",
    "                total_reward = 0.0\n",
    "                while not done:\n",
    "                    action = agent.get_action(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    next_obs = np.reshape(next_obs, [1, state_size])\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшения метода Policy Gradients\n",
    "\n",
    "Такого большого набора твиков, как для DQN для DPG не наблюдается. Наиболее известны методы: TRPO (Trust Region Policy Optimization) и PPO (Proximal Policy Optimization). В них заложена несколько разная математика, но суть обоих методов в ограничении изменения весов за один прогон для того, чтобы выбросы не портили выученную стратегию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Комбинирование подходов\n",
    "\n",
    "## Actor-critic\n",
    "\n",
    "Можно взять две сети, одна из которых будет предсказывать действия, а вторая - оценивать, насколько эти действия хороши, т.е. выдавать значение Q. Помимо самих действий ей на вход мы так же подадим состояне.\n",
    "\n",
    "<img src=\"actor_critic.png\">\n",
    "\n",
    "Плюс в том, что нам не обязательно дожидаться окончания эпизода для обучения.\n",
    "\n",
    "## Advantage-Actor-Critic, A2C\n",
    "\n",
    "Вместо вычисления градиентов от абсолютного значения \\\\( Q(s, a) \\\\) мы можем использовать относительное преимущество одний действия над другими \\\\( A(s, a) = Q(s, a) - V(s) \\)). Тогда если \\\\( A(s, a) > 0 \\\\), то градиентный спуск будет изменять все веса, повышая вероятность предсказанных действий. Если же \\\\( A(s, a) < 0 \\\\), то градиентный спуск будет понижать вероятность таких действий. \\\\( V(s) \\\\) при этом показывает, насколько состояние хорошо само по себе: если мы в двух шагах от вершины Эвереста, то это очень хороший state, а если мы летим в пропасть, то state крайне фиговый, чтобы мы в нём не делали (если, конечно, у нас нет с собой парашюта).\n",
    "\n",
    "При использовании такого подхода функцию Q(s, a) можно заменить прямо на полученное вознаграждение за некоторое действие вознаграждение r. Тогда \\\\( A(s,a) = r - V(s) \\\\). При этом получается, что сети Actor и Critic можно объединить в одну с двумя головами, что улучшает переиспользование весов и ускоряет обучение.\n",
    "\n",
    "<img src=\"a2c.png\" width=700>\n",
    "\n",
    "A3C или Asynchronous Advantage Actor Critic - означает, что у нас есть сервер, собирающий результаты с нескольких Actor'ов и обновляющий веса, когда наберётся батч достаточного размера. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 147\n",
      "Trainable params: 147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 20.004807747138898\n",
      "episode: 1   score: 36.29626079719125\n",
      "episode: 2   score: 48.75975777888116\n",
      "episode: 3   score: 28.011833044320696\n",
      "episode: 4   score: 39.95116257572125\n",
      "episode: 5   score: 25.334604009190965\n",
      "episode: 6   score: 38.13629999673902\n",
      "episode: 7   score: 40.23745549595444\n",
      "episode: 8   score: 24.384500410876964\n",
      "episode: 9   score: 43.77279450822117\n",
      "episode: 10   score: 34.426514895317545\n",
      "episode: 11   score: 35.83312817670101\n",
      "episode: 12   score: 22.954665682620668\n",
      "episode: 13   score: 29.017358720800313\n",
      "episode: 14   score: 48.56698777910014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5399986acb80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbest_velocity\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_height\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbest_height\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-5399986acb80>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 701\n",
    "\n",
    "\n",
    "# A2C(Advantage Actor-Critic) agent for the Cartpole\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = True\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.0001\n",
    "        self.critic_lr = 0.0005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"./save_model/cartpole_actor.h5\")\n",
    "            self.critic.load_weights(\"./save_model/cartpole_critic.h5\")\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = gym.make(envName)\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # make A2C agent\n",
    "    agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            best_height = next_state[0][0]+0.5\n",
    "            best_velocity = abs(next_state[0][1])\n",
    "            reward = 20*best_velocity + abs(best_height) + 0.0001*best_height\n",
    "            \n",
    "            agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode, plot the play time\n",
    "                #score = score if score == 500.0 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_a2c.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > -10:\n",
    "                    break\n",
    "\n",
    "        # save the model\n",
    "        if e % 25 == 0:\n",
    "            agent.actor.save_weights(\"./save_model/cartpole_actor2.h5\")\n",
    "            agent.critic.save_weights(\"./save_model/cartpole_critic2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_250 (Dense)            (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 147\n",
      "Trainable params: 147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_252 (Dense)            (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        env = gym.make(envName)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        agent = A2CAgent(state_size, action_size)\n",
    "        agent.load_model = 1\n",
    "        agent.epsilon = 0\n",
    "\n",
    "        # Use the Gym Monitor wrapper to evaluate the agent and record video\n",
    "        gym_monitor_path = \"./gym_monitor_output\"\n",
    "        env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n",
    "        for _ in range(10):\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                obs = np.reshape(obs, [1, state_size])\n",
    "                total_reward = 0.0\n",
    "                while not done:\n",
    "                    action = agent.get_action(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    next_obs = np.reshape(next_obs, [1, state_size])\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based\n",
    "\n",
    "Ещё один подход заключается в том, что мы можем обучать нейросеть предсказывать следующее состояние среды, подавая ей на вход действия и предыдущее состояние. Таким образом нейросеть учит поведение среды. Для того, чтобы выбрать оптимальные действия, нам придётся прогнать все возможные действия через предсказание нейросети, поэтому такой подход применим только при малой размерности пространства действий.\n",
    "\n",
    "# Imitation learning\n",
    "\n",
    "Для того, чтобы агент выучил сложную последовательность действий, можно искуственно поставить его в конец этой траектории, тогда он быстро выучит, как пройти небольшой участок. После этого его можно поставить чуть дальше и так, пока агент на научится выполнять всю последовательность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo:\n",
    "- DQN - проанализировать фиттинг и чекнуть тест; разобраться с донастройкой\n",
    "- RDQN - проанализировать фиттинг, обновить функцию потерь и дообучить\n",
    "- А2С - разобраться и доучить"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
